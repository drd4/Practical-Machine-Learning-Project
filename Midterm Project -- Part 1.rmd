---
title: "Data Science Consulting: Midterm Team Project -- Part 1"
author: ""
date: ""
output: html_document
---

```{r setup, include=FALSE}
set.seed(72)
knitr::opts_chunk$set(echo = TRUE, comment="", warning = FALSE, message = FALSE, tidy.opts=list(width.cutoff=55))
```

```{r libraries, echo = FALSE}
library(data.table)
library(DT)
library(randomForest)
library(caret)
library(rpart)
library(class)
library(e1071)
library(xgboost)
library(magrittr)
library(dplyr)
library(Matrix)
library(e1071)
library(nnet)
library(gbm)
```

```{r source_files}

```

```{r functions}
sample_split = function(data,size,n){
  sample_data <- data[sample(1:nrow(data), size*3), ]
  sample_split <-split(sample_data, 
                       rep(1:3, length.out = nrow(sample_data), 
                           each = ceiling(nrow(sample_data)/3)))
  return(sample_split[[n]])
}
```

```{r constants}
n.values <- c(5000, 10000, 20000)
iterations <- 3
```

```{r load_data}
train <- read.csv("MNIST-fashion training set-49.csv")
test <- read.csv("MNIST-fashion testing set-49.csv")
```

```{r clean_data}
# change label to factor
train$label = factor(train$label)
test$label = factor(test$label)

# normalize data
train[,-1] = scale(train[,-1])
test[,-1] = scale(test[,-1])
```

```{r generate_samples}

```

## Introduction {.tabset}


### Model 1



```{r code_model1_development, eval = TRUE}
Model1 <- function(data){
  tree <- rpart(label ~ ., data = data, method = 'class')
  pred <- predict(tree, newdata = test, type = "class")
  return(pred)
}
```

```{r load_model1}

```

### Model 2


```{r code_model2_development, eval = TRUE}
Model2 <- function(data){
  #trControl = trainControl(method = 'cv', number = 49)
  #tuneGrid = expand.grid(mtry = 2:49)
  #Model = train(label ~ ., data = sample, method = 'rf', ntree = ntree, trControl = trControl, tuneGrid = tuneGrid)
  model = randomForest(label ~ ., data = data, ntree=50, mtry=2, importance=TRUE,proximity = TRUE)
  pred = predict(model, newdata = test)
  return(pred)
}
```

```{r load_model2}

```

### Model 3


```{r code_model3_development, eval = TRUE}
Model3 <- function(data){
  model = randomForest(label ~ ., data = data, ntree=100, mtry=3, importance=TRUE,proximity = TRUE)
  pred = predict(model, newdata = test)
  return(pred)
}
```

```{r load_model3}

```

### Model 4


```{r code_model4_development, eval = TRUE}
Model4<- function(data) {
  mlr.model = multinom(label ~.,
                       data = data,trace = F)
  
  pred = predict(mlr.model, newdata = test)
  return(pred)
}
```

```{r load_model4}

```

### Model 5


```{r code_model5_development, eval = TRUE}
Model5 <- function(data) {
  pred = knn(data[, -1], test[, -1], data$label, k = 5)
  return(pred)
}
```

```{r load_model5}

```

### Model 6


```{r code_model6_development, eval = TRUE}
Model6 = function(data){
  model = svm(as.factor(label) ~ ., data = data, kernel = "linear", scale = T)
  pred = predict(model, test[,-1])
  return(pred)
}
```

```{r load_model6}

```

### Model 7


```{r code_model7_development, eval = TRUE}
Model7 <- function(data){
  model  <- gbm(label ~ ., data = data, 
                 distribution = "multinomial",n.trees = 25, 
                 shrinkage = 0.01, interaction.depth = 4)
  pred = predict(model, newdata=test)
  pred = as.data.frame(pred)
  colnames(pred) = levels(as.factor(test$label))
  pred$prediction = apply(pred,1,function(x) colnames(pred)[which.max(x)])
  return(pred$prediction)
}

```

```{r load_model7}

```

### Model 8


```{r code_model8_development, eval = TRUE}
Model8 <- function(data){
  model  <- gbm(label ~ ., data = data, 
                 distribution = "multinomial",n.trees = 50, 
                 shrinkage = 0.01, interaction.depth = 4)
  pred = predict(model, newdata=test)
  pred = as.data.frame(pred)
  colnames(pred) = levels(as.factor(test$label))
  pred$prediction = apply(pred,1,function(x) colnames(pred)[which.max(x)])
  return(pred$prediction)
}
```

```{r load_model8}

```

### Model 9


```{r code_model9_development, eval = TRUE}
Model9 = function(data){
  train.label = as.integer(as.factor(data$label))-1
  train_matrix = as.matrix(data[,-1])
  test.label = as.integer(as.factor(test$label))-1
  test_matrix = as.matrix(test[,-1])
  xgb.train = xgb.DMatrix(data=train_matrix,label=train.label)
  xgb.test = xgb.DMatrix(data=test_matrix,label=test.label)
  model = xgboost(data = xgb.train, max.depth = 20, eta = 0.01, 
                  nthread = 2, nrounds = 2, num_class = length(unique(data$label)), 
                  objective = "multi:softprob", eval_metric="mlogloss")
  pred <- predict(model, newdata = xgb.test,reshape=T)
  pred = as.data.frame(pred)
  colnames(pred) = levels(as.factor(test$label))
  pred$prediction = apply(pred,1,function(x) colnames(pred)[which.max(x)])
  pred$label = levels(as.factor(test$label))[test.label+1]
  return(pred$prediction)
}
```

```{r load_model9}

```

### Model 10


```{r code_model10_development, eval = TRUE}
Model10 = function(data){
  train.label = as.integer(as.factor(data$label))-1
  train_matrix = as.matrix(data[,-1])
  test.label = as.integer(as.factor(test$label))-1
  test_matrix = as.matrix(test[,-1])
  xgb.train = xgb.DMatrix(data=train_matrix,label=train.label)
  xgb.test = xgb.DMatrix(data=test_matrix,label=test.label)
  model = xgboost(data = xgb.train, max.depth = 40, eta = 0.01, 
                  nthread = 2, nrounds = 2, num_class = length(unique(data$label)), 
                  objective = "multi:softprob", eval_metric="mlogloss")
  pred <- predict(model, newdata = xgb.test,reshape=T)
  pred = as.data.frame(pred)
  colnames(pred) = levels(as.factor(test$label))
  pred$prediction = apply(pred,1,function(x) colnames(pred)[which.max(x)])
  pred$label = levels(as.factor(test$label))[test.label+1]
  return(pred$prediction)
}
```

```{r load_model10}

```

## Scoreboard

* Model 1: Classification Tree
* Model 2: Random Forest with 50 trees and 2 mtry
* Model 3: Random Forest with 100 trees and 3 mtry
* Model 4: Multinomial Logistic Regression
* Model 5: K-Nearest Neighbors
* Model 6: Support Vector Machines
* Model 7: Generalized Boosted Regression Models with 25 trees
* Model 8: Generalized Boosted Regression Models with 50 trees
* Model 9: Extreme Gradient Boosting Regression Models with max.depth = 20
* Model 10: Extreme Gradient Boosting Regression Models with max.depth = 40


```{r scoreboard}
models = c(Model1,Model2,Model3,Model4,Model5,Model6,Model7,Model8,Model9,Model10)

scoreboard = data.frame()

for (k in 1:length(models)) {
  for (i in 1:length(n.values)) {
    for (j in 1:iterations) {
      train_data = sample_split(train, n.values[i], j)
      start_time <- Sys.time()
      pred = models[[k]](train_data)
      end_time <- Sys.time()
      sys_time = end_time - start_time
      Model = paste('Model',k)
      Data = paste('dat_',n.values[i],'_',j, sep = '')
      A = n.values[i]/60000
      B = min(1,sys_time/60)
      C = sum(test$label != pred)/NROW(test$label)
      Points = 0.15 * A + 0.1 * B + 0.75 * C
      score_row = data.frame(Model, 'Sample Size' = n.values[i], Data, A, B, C, Points)
      scoreboard = rbind(scoreboard,score_row)
  }
  }}
datatable(scoreboard)

```

**scoreboard summary**

```{r scoreboard summary}
scoreboard_summary = scoreboard %>% group_by(Model,Sample.Size) %>% 
  summarize(A = round(mean(A),4), B = round(mean(B),4), C = round(mean(C),4), Points = round(mean(Points),4)) %>% 
  arrange(Points)
datatable(scoreboard_summary)
```



## Discussion


## Model Development Responsibilities

For the 10 models, please list the names of the developers along with percentages for how the responsibilities were divided.

1. Phoebe
2. Phoebe
3. Phoebe
4. Daniel
5. Daniel
6. Haodong
7. Haodong
8. Devin
9. Haodong
10. Devin

## References


