MODEL REPORT

1. Introduction

In this project, 10 machine learning models were created to predict 10 types of product labels based on 49 color pixels (7x7 images). For each model, 60,000 rows of training data were used and the models were tested on 10,000 rows of testing data. The 10 types of product labels the models were trained to predict are: T-shirt/top Trouser, Pullover, Dress, Coat, Sandal, Shirt, Sneaker, Bag, and Ankle boot. Each color pixel is recorded as a brightness measurement in grayscale with 0 denoting white and 255 denoting black. The details of how each parameter for each model was chosen is described in the Discussion section of this report below. 

2. Discussion

Model 1: Classification Tree - Phoebe
Since the variable being predicted (product label) is a categorical variable and classification trees are used to predict categorical variables, we chose to build a classification tree for our first model. A classification tree predictsthat each obseravtion belongs to the most commonly occurring class of training observations in the region to which it belongs. This model performed well in term of run time as all run-times were near 0 seconds. However, with no parameters, this model did not yield the highest accuracy compared to other models. Overall, a classification tree is a quick and easy model to fit for predicting categorical outcomes, but may not yield the highest accuracy compared to other machine learning models.

Model 2 & 3: Random Forest - Phoebe
Random Forest models reduces the variance when averaging a tree by decorrelating the tree. Specifically, a random selection of n predictors is chosen as split candidates each time a split happens and only 1 of the n predictors is used as the split. A new selection of n predictors is chosen at each split. This ensures that not all trees will use the strongest predictor as the first split, which can thus reduce the variance of a tree. Because of this technique, random forests can generate more accurate predictions than other trees (such as classification trees), but as the training sample increases in size and as there are more parameters to tune, random forests can take longer to run.

For our random forest model, a tuneGrid was used to fit from 2 to 49 variables (since there are 49 pixels). From this tuneGrid, it was found that a mtry of 24 yielded the best model so this parameter was used for all sample sizes. Once this was determined, this parameter was used in bagging in order to identify the optimal number of trees to include in the model. After plotting the bags for each sample size, it was determined that the best number of trees is between 20 and 25. Thus, we createed 2 random tree models that used 20 trees and 25 trees respectively. 

